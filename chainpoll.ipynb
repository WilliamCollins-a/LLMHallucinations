{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install what needs to be installed setting up instance here\n",
    "\n",
    "#Google Drive mount here\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ab91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up file location here\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/LLMHallucination')\n",
    "\n",
    "# import libraries\n",
    "from Util import load_data_JSON, load_model, write_out\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7928e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "general_df, prompt_g, correct_g, gtp4_g, Info_g = load_data_JSON('input/general_data.json', 'general')\n",
    "qa_df, prompt_q, correct_q, gtp4_q, Info_q = load_data_JSON('input/qa.json', 'qa')\n",
    "sum_df, prompt_s, correct_s, gtp4_s, Info_s = load_data_JSON('input/summarization_data.json', 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2bae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access input (API key, etc.)\n",
    "access = input('Access code?')\n",
    "\n",
    "#setup of model\n",
    "tokenizer, model = load_model(\"meta-llama/Llama-2-7b-chat-hf\", access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChainPoll method\n",
    "def chainpoll(model, tokenizer, prompts, num_responses=5):\n",
    "    # Function to detect hallucinations using ChainPoll\n",
    "    out_q = []\n",
    "\n",
    "    # Process the questions (currently the first 2 questions)\n",
    "    for i, prompt in enumerate(prompts[:2]):\n",
    "        hallucination_scores = []\n",
    "\n",
    "        # Run 5 times for the ChainPoll method\n",
    "        for _ in range(num_responses):\n",
    "            # Tokenize the prompt for input\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "            # Generate the response from the model\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs['input_ids'],\n",
    "                    max_length=50,  # Adjust based on needs\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "            # Decode the output response\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Ask the model if there are hallucinations using the CoT method\n",
    "            hallucination_prompt = f\"Does the following ouput contain hallucinations? Explain in detail:\\n\\nOutput: {response}\\n\"\n",
    "            hallucination_inputs = tokenizer(hallucination_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "            # Generate the hallucination response\n",
    "            with torch.no_grad():\n",
    "                hall_outputs = model.generate(\n",
    "                    hallucination_inputs['input_ids'],\n",
    "                    max_length=100,  # Adjust this as needed to allow space for CoT\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "            # Decode the hallucination response\n",
    "            hallucination_response = tokenizer.decode(hall_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Check for \"yes\" or \"no\" in the hallucination response and store the score\n",
    "            if \"yes\" in hallucination_response.lower():\n",
    "                hallucination_scores.append(1)\n",
    "            else:\n",
    "                hallucination_scores.append(0)\n",
    "\n",
    "        # Compute final hallucination score as the proportion of \"yes\" responses\n",
    "        hallucination_score = sum(hallucination_scores) / num_responses\n",
    "\n",
    "        # Save the final hallucination judgment and explanation\n",
    "        out_q.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "            \"hallucination_score\": hallucination_score,\n",
    "            \"explanation\": hallucination_response\n",
    "        })\n",
    "\n",
    "    # Write the results\n",
    "    write_out(\"ChainPoll_qa_results.csv\", out_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc335dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the ChainPoll method\n",
    "chainpoll(model, tokenizer, prompt_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run of the prompts for the general section\n",
    "out_g = []\n",
    "for p in prompt_g:\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=50,  # Adjust this based on your needs\n",
    "            num_beams=5,  # Optional: Use beam search to improve generation quality\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    out_g.append(response)\n",
    "\n",
    "write_out(\"Baseline_control_general.csv\", out_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run of the prompts for summarization\n",
    "out_s = []\n",
    "for p in prompt_s:\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(\"Summarize this document: \" + p, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=500,  # Adjust this based on your needs\n",
    "            num_beams=5,  # Optional: Use beam search to improve generation quality\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    out_s.append(response)\n",
    "\n",
    "write_out(\"Baseline_control_summarisation.csv\", out_s)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
