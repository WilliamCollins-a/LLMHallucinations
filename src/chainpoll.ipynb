{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897cd479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chainpoll User Guide\n",
    "\n",
    "# How to run:\n",
    "\n",
    "# 1. Open Google Colab and ensure you're signed in to your Google account.\n",
    "# 2. Save the required files (general_data.json, qa_data.json, summarization_data.json, and Util.py) in your Google Drive under MyDrive.\n",
    "# 3. Upload the chainpoll.ipynb file to Google Colab and open.\n",
    "# 4. Change runtime type to T4 (GPU) for faster model processing. \n",
    "# 5. Click \"Run\" on each cell to execute the code sequentially.\n",
    "#     - The code will:\n",
    "#         - Install necessary packages\n",
    "#         - Mount Google Drive to access files\n",
    "#         - Load the model and data files from Google Drive\n",
    "#         - Use the ChainPoll method to assess model responses for hallucinations\n",
    "#         - Save results to a CSV file in Google Drive\n",
    "# 6. Once complete, open your Google drive and locate the Chainpoll_qa_results.csv file. This file stores the prompts, responses, hallucination scores, and explanations in CSV format.\n",
    "#    This can be opened in any spreadsheet software for analysis. \n",
    "\n",
    "# Note:\n",
    "# - When prompted, enter your API key or access code for model access.\n",
    "# - Adjust the for i, prompt in enumerate(prompts[1:25]): section to adjust how many prompts are ran for the qa_df dataset.\n",
    "# - After execution, check your Google Drive (MyDrive) for \"ChainPoll_qa_results.csv\" to review the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall outdated library versions\n",
    "!pip uninstall -y bitsandbytes accelerate\n",
    "\n",
    "# Reinstall updated versions of required libraries\n",
    "%pip install -U bitsandbytes accelerate\n",
    "import bitsandbytes as bnb\n",
    "import accelerate\n",
    "import csv\n",
    "\n",
    "# Mount Google Drive account to access files saved in the drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enabling import of custom module for colab\n",
    "! cp /content/gdrive/MyDrive/Util.py . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ab91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file location here\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/MyDrive')\n",
    "\n",
    "# Import essential libraries used to run Chainpoll\n",
    "from Util import load_data_JSON, load_model, write_out\n",
    "import pandas as pd\n",
    "import json\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7928e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from JSON files for all tasks. Note, the qa_df is required only but consistency, all files are required. \n",
    "general_df, prompt_g, correct_g, gtp4_g, Info_g = load_data_JSON('/content/gdrive/MyDrive/general_data.json', 'general')\n",
    "qa_df, prompt_q, correct_q, gtp4_q, Info_q = load_data_JSON('/content/gdrive/MyDrive/qa_data.json', 'qa')\n",
    "sum_df, prompt_s, correct_s, gtp4_s, Info_s = load_data_JSON('/content/gdrive/MyDrive/summarization_data.json', 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2bae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for the user to input access code (API key, etc.)\n",
    "access = input('Access code?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the specified model and tokenizer using the access code for any necessary authentication\n",
    "tokenizer, model = load_model(\"meta-llama/Llama-2-7b-chat-hf\", access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a text generation pipeline with the model and tokenizer, using half-precision floats for efficiency\n",
    "gen = pipeline(\"text-generation\", model=model, torch_dtype=torch.float16, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4718851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a single response for a given prompt\n",
    "def get_response(prompt, max_len):\n",
    "    sequences = gen(prompt, do_sample=True, top_k=5, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, max_length=max_len + len(prompt))\n",
    "    return sequences[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChainPoll method to assess responses for hallucinations\n",
    "def chainpoll(model, tokenizer, prompts, num_responses=5):\n",
    "    out_q = []\n",
    "\n",
    "    # Loop through an amount of prompts (first 25 prompts)\n",
    "    for i, prompt in enumerate(prompts[1:25]): # Adjust this based on how many prompts for the qa_df dataset you want to run\n",
    "        hallucination_scores = []\n",
    "\n",
    "        # Generate multiple responses per prompt to check for hallucinations\n",
    "        for _ in range(num_responses):\n",
    "            # Tokenize the prompt for input\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "            # Generate the response from the model\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs['input_ids'],\n",
    "                    max_new_tokens=50,  # Restrict the response length\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "            # Decode the output response\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Ask the model if there are hallucinations using the CoT method\n",
    "            hallucination_prompt = f\"Does the following output contain hallucinations? Explain in detail:\\n\\nOutput: {response}\\n\"\n",
    "            hallucination_inputs = tokenizer(hallucination_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "            # Generate the hallucination response\n",
    "            with torch.no_grad():\n",
    "                hall_outputs = model.generate(\n",
    "                    hallucination_inputs['input_ids'],\n",
    "                    max_new_tokens=100,  # Allow enough tokens for a full explanation\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "            # Decode the hallucination response\n",
    "            hallucination_response = tokenizer.decode(hall_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Check for \"yes\" or \"no\" in the hallucination response and store the score\n",
    "            if \"yes\" in hallucination_response.lower():\n",
    "                hallucination_scores.append(1)\n",
    "            else:\n",
    "                hallucination_scores.append(0)\n",
    "\n",
    "        # Compute final hallucination score as the proportion of \"yes\" responses\n",
    "        hallucination_score = sum(hallucination_scores) / num_responses\n",
    "\n",
    "         # Store the result with prompt, response, hallucination score, and explanation\n",
    "        out_q.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "            \"hallucination_score\": hallucination_score,\n",
    "            \"explanation\": hallucination_response\n",
    "        })\n",
    "\n",
    "    return out_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875df8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the chainpoll function on the question and answering prompts\n",
    "chainpoll_results = chainpoll(model, tokenizer, prompt_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and format the chainPoll results\n",
    "out_q = []\n",
    "for result in chainpoll_results:\n",
    "    prompt = result[\"prompt\"]\n",
    "    response = result[\"response\"]\n",
    "    hallucination_score = result[\"hallucination_score\"]\n",
    "    explanation = result[\"explanation\"]\n",
    "\n",
    "    # Create a formatted string for each result\n",
    "    formatted_output = f\"\"\"\n",
    "    Prompt: {prompt}\n",
    "\n",
    "    Response: {response}\n",
    "\n",
    "    Hallucination Score: {hallucination_score}\n",
    "\n",
    "    Explanation: {explanation}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Append results to out_q for writing\n",
    "    out_q.append(formatted_output)\n",
    "\n",
    "# Optional - Print out the formatted results for checking (remove #'s)\n",
    "# for formatted_result in out_q:\n",
    "#    print(formatted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d498aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the formatted results to a CSV file in Google Drive\n",
    "def write_out(filename, data):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile, delimiter=',')  # Use a comma delimiter\n",
    "        csv_writer.writerows([elt] for elt in data)\n",
    "\n",
    "write_out(\"/content/gdrive/MyDrive/ChainPoll_qa_results.csv\", out_q)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
