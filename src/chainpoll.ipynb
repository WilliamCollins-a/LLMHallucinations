{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install what needs to be installed setting up instance here\n",
    "!pip uninstall -y bitsandbytes accelerate\n",
    "%pip install -U bitsandbytes accelerate\n",
    "%pip install langdetect\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import accelerate\n",
    "from langdetect import detect\n",
    "\n",
    "#Google Drive mount here\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enabling import of custom module for colab\n",
    "! cp /content/gdrive/MyDrive/Util.py . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ab91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up file location here\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/MyDrive')\n",
    "\n",
    "# import libraries\n",
    "from Util import load_data_JSON, load_model, write_out\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7928e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "general_df, prompt_g, correct_g, gtp4_g, Info_g = load_data_JSON('/content/gdrive/MyDrive/general_data.json', 'general')\n",
    "qa_df, prompt_q, correct_q, gtp4_q, Info_q = load_data_JSON('/content/gdrive/MyDrive/qa_data.json', 'qa')\n",
    "sum_df, prompt_s, correct_s, gtp4_s, Info_s = load_data_JSON('/content/gdrive/MyDrive/summarization_data.json', 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2bae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access input (API key, etc.)\n",
    "access = input('Access code?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup of model\n",
    "tokenizer, model = load_model(\"meta-llama/Llama-2-7b-chat-hf\", access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pipeline(\"text-generation\", model=model,torch_dtype = torch.float16, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4718851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt to enforce English-only responses\n",
    "system_prompt = \"\"\"[INST] <>\n",
    "You are a helpful assistant. Please ensure all responses are in clear and fluent English, without using any other languages.\n",
    "<>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the text is in English\n",
    "def is_english(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang == 'en'\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604576ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response without retries, enforce English via system prompt\n",
    "def get_response(prompt, max_len):\n",
    "    prompt_with_system = system_prompt + prompt + \" [/INST]\"\n",
    "    \n",
    "    sequences = gen(prompt_with_system, do_sample=True, top_k=5, num_return_sequences=1, \n",
    "                    eos_token_id=tokenizer.eos_token_id, max_length=max_len + len(prompt))\n",
    "    response = sequences[0]['generated_text']\n",
    "\n",
    "    # Detect language of the response\n",
    "    if is_english(response):\n",
    "        return response\n",
    "    else:\n",
    "        return \"Non-English response detected.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chainpoll(model, tokenizer, prompts, num_responses=5):\n",
    "    out_q = []\n",
    "\n",
    "    for i, prompt in enumerate(prompts[:2]):\n",
    "        hallucination_scores = []\n",
    "        explanations = []\n",
    "\n",
    "        prompt_with_system = system_prompt + f\"{prompt} [/INST]\"\n",
    "\n",
    "        for _ in range(num_responses):\n",
    "            inputs = tokenizer(prompt_with_system, return_tensors=\"pt\").to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs['input_ids'],\n",
    "                    max_new_tokens=50,\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Ask about hallucinations for each response\n",
    "            hallucination_prompt = f\"Does the following output contain hallucinations? Explain in detail:\\n\\nOutput: {response}\\n\"\n",
    "            hallucination_inputs = tokenizer(hallucination_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                hall_outputs = model.generate(\n",
    "                    hallucination_inputs['input_ids'],\n",
    "                    max_new_tokens=100,\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            hallucination_response = tokenizer.decode(hall_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Evaluate hallucination score based on the explanation (if \"yes\" or hallucinations are detected)\n",
    "            if \"yes\" in hallucination_response.lower():\n",
    "                hallucination_scores.append(1)\n",
    "            else:\n",
    "                hallucination_scores.append(0)\n",
    "            \n",
    "            explanations.append(hallucination_response)\n",
    "\n",
    "        # Calculate average hallucination score for this prompt\n",
    "        avg_hallucination_score = sum(hallucination_scores) / num_responses\n",
    "\n",
    "        # Append result with average score and explanations\n",
    "        out_q.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,  # Only the final response will be saved (can change if needed)\n",
    "            \"hallucination_score\": avg_hallucination_score,\n",
    "            \"explanation\": explanations  # All explanations will be stored\n",
    "        })\n",
    "\n",
    "    return out_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ensure output is in English before formatting\n",
    "def format_output_english_only(chainpoll_results):\n",
    "    formatted_out_q = []\n",
    "    \n",
    "    for result in chainpoll_results:\n",
    "        # Check if response is in English\n",
    "        if is_english(result['response']):\n",
    "            formatted_output = f\"\"\"\n",
    "            Prompt: {result['prompt']}\n",
    "\n",
    "            Response: {result['response']}\n",
    "\n",
    "            Hallucination Score: {result['hallucination_score']}\n",
    "\n",
    "            Explanation: {result['explanation']}\n",
    "            \"\"\"\n",
    "            formatted_out_q.append(formatted_output)\n",
    "        else:\n",
    "            # Optional: If you want to flag non-English responses\n",
    "            print(f\"Non-English response detected, skipping:\\n{result['response']}\\n\")\n",
    "    \n",
    "    return formatted_out_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875df8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "chainpoll_results = chainpoll(model, tokenizer, prompt_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36401d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the output\n",
    "formatted_out_q = format_output_english_only(chainpoll_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d498aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results\n",
    "write_out(\"/content/gdrive/MyDrive/ChainPoll_qa_results.csv\", formatted_out_q)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
