{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamCollins-a/LLMHallucinations/blob/main/Knowhalu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aAr-3UsDV2Or"
      },
      "outputs": [],
      "source": [
        "#install what needs to be installed setting up intance here\n",
        "!pip install -U bitsandbytes\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiTWLOSVV2PA"
      },
      "outputs": [],
      "source": [
        "#Google Drive mount here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6TJYSFpV2PC"
      },
      "outputs": [],
      "source": [
        "#set up file location here\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/LLMHallucination')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9V1ssBYV2PE"
      },
      "outputs": [],
      "source": [
        "#enabling import of custom module for colab\n",
        "! cp /content/gdrive/MyDrive/LLMHallucinations/Util.py .\n",
        "\n",
        "# import libraries\n",
        "from Util import load_data_JSON, load_model, write_out\n",
        "import pandas as pd\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-ePyphkV2PG"
      },
      "outputs": [],
      "source": [
        "#load the data\n",
        "\n",
        "general_df,prompt_g,correct_g,gtp4_g,Info_g = load_data_JSON('/content/gdrive/MyDrive/LLMHallucinations/input/general_data.json','general')\n",
        "qa_df,prompt_q,correct_q,gtp4_q,Info_q = load_data_JSON('/content/gdrive/MyDrive/LLMHallucinations/input/qa_data.json','qa')\n",
        "sum_df,prompt_s,correct_s,gtp4_s,Info_s = load_data_JSON('/content/gdrive/MyDrive/LLMHallucinations/input/summarization_data.json','sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XulLOm6AV2PH",
        "outputId": "bb3ee5a3-37b8-44d2-d1e1-af747d0c9b6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Access code?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\n"
          ]
        }
      ],
      "source": [
        "# Access input (API key, etc.)\n",
        "access = input('Access code?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "veADYT_BV2PJ"
      },
      "outputs": [],
      "source": [
        "#setup of model\n",
        "tokenizer, model = load_model(\"meta-llama/Llama-2-7b-chat-hf\",access)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def listOfTuples(l1, l2):\n",
        "    return list(map(lambda x, y:(x,y), l1, l2))"
      ],
      "metadata": {
        "id": "KoyIpXCc6lZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method definition_pipeline"
      ],
      "metadata": {
        "id": "Ey3NoJwapPmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = pipeline(\"text-generation\", model=model,torch_dtype = torch.float16, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "zXhdNGMqoo--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(prompt,max_len):\n",
        "  sequences = gen(prompt,do_sample=True,top_k = 5, num_return_sequences =1, eos_token_id = tokenizer.eos_token_id, max_length = max_len +len(prompt) )\n",
        "  return (sequences[0]['generated_text'])"
      ],
      "metadata": {
        "id": "9j3G3LWmopXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run of the prompts_1\n",
        "out_q =[]\n",
        "qp = listOfTuples(prompt_q,Info_q)\n",
        "for p in qp:\n",
        "    # Decode the output\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    prompt_qa =  \"You are a huallucination detector. You MUST determine if the provided answer contains hallucination or not for the question based on the provided Knowledge. #Question#: {question} #Answer#: {answer} #Your Judgement#:\"\n",
        "    inputs = tokenizer(prompt_qa, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate the response\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=50,\n",
        "            num_beams=5,\n",
        "            early_stopping=True)\n",
        "\n",
        "    # Decode the output\n",
        "    response2 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    out_q.append(response,response2)\n",
        ""
      ],
      "metadata": {
        "id": "mfu8zXxOTq4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Second alternative_use_pipeline"
      ],
      "metadata": {
        "id": "HPLbFu0BodoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_q =[]\n",
        "system_prompt = \"\"\"<s>[INST] <<SYS>>\n",
        "You are a huallucination detector. You MUST determine if the provided answer contains hallucination or not for the question based on the provided Knowledge. #Question#: {question} #Answer#: {answer} #Your Judgement#:  Your answers are clear and concise.Your answers are in English.\n",
        "<</SYS>>\n",
        "\n",
        "\"\"\"\n",
        "for p in prompt_q:\n",
        "    prompt = system_prompt + f\"{p} [/INST]\"\n",
        "    response = get_response(prompt,256)\n",
        "    response = response[len(prompt):]\n",
        "    out_q.append(response)"
      ],
      "metadata": {
        "id": "HASO4SWmouSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_out(\"/content/gdrive/MyDrive/LLMHallucinations/Baseline_control_qa.csv\",out_q)"
      ],
      "metadata": {
        "id": "FvVifyIlou3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdszZoYfcUdI"
      },
      "outputs": [],
      "source": [
        "#function to get response which is used for evaluation_qa_dataset()\n",
        "def get_qa_response(model, question, answer, instruction):\n",
        "    message = [\n",
        "        {\"role\": \"system\", \"content\":\"You are a huallucination detector. You MUST determine if the provided answer contains hallucination or not for the question based on the world knowledge. The answer you provided MUST be \\\"Yes\\\" or \\\"No\\\"\"},\n",
        "        {\"role\": \"user\", \"content\": instruction +\n",
        "                                    \"\\n\\n#Question#: \" + question +\n",
        "                                    \"\\n#Answer#: \" + answer +\n",
        "                                    \"\\n#Your Judgement#: \"}\n",
        "    ]\n",
        "    prompt = instruction + \"\\n\\n#Question#: \" + question + \"\\n#Answer#: \" + answer + \"\\n#Your Judgement#:\"\n",
        "    while True:\n",
        "        try:\n",
        "            if model == \"gpt-3.5-turbo\":\n",
        "                res = openai.ChatCompletion.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=message,\n",
        "                    temperature=0.0,\n",
        "                )\n",
        "                response = res['choices'][0]['message']['content']\n",
        "            else:\n",
        "                res = openai.Completion.create(\n",
        "                    engine=model,\n",
        "                    prompt=prompt,\n",
        "                    temperature=0.0\n",
        "                )\n",
        "                response = res[\"choices\"][0]['text'].strip()\n",
        "            break\n",
        "        except openai.error.RateLimitError:\n",
        "            print('openai.error.RateLimitError\\nRetrying...')\n",
        "            time.sleep(60)\n",
        "        except openai.error.ServiceUnavailableError:\n",
        "            print('openai.error.ServiceUnavailableError\\nRetrying...')\n",
        "            time.sleep(20)\n",
        "        except openai.error.Timeout:\n",
        "            print('openai.error.Timeout\\nRetrying...')\n",
        "            time.sleep(20)\n",
        "        except openai.error.APIError:\n",
        "            print('openai.error.APIError\\nRetrying...')\n",
        "            time.sleep(20)\n",
        "        except openai.error.APIConnectionError:\n",
        "            print('openai.error.APIConnectionError\\nRetrying...')\n",
        "            time.sleep(20)\n",
        "\n",
        "    return response\n",
        "\n",
        "#Evaluate response\n",
        "def evaluation_qa_dataset(model, file, instruction, output_path):\n",
        "    with open(file, 'r', encoding=\"utf-8\") as f:\n",
        "        data = []\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "        correct = 0\n",
        "        incorrect = 0\n",
        "        for i in range(len(data)):\n",
        "            knowledge = data[i][\"knowledge\"]\n",
        "            question = data[i][\"question\"]\n",
        "            hallucinated_answer = data[i][\"hallucinated_answer\"]\n",
        "            right_answer = data[i][\"right_answer\"]\n",
        "\n",
        "            if random.random() > 0.5:\n",
        "                answer = hallucinated_answer\n",
        "                ground_truth = \"Yes\"\n",
        "            else:\n",
        "                answer = right_answer\n",
        "                ground_truth = \"No\"\n",
        "\n",
        "            ans = get_qa_response(model, question, answer, instruction)\n",
        "            ans = ans.replace(\".\", \"\")\n",
        "\n",
        "            if (\"Yes\" in ans and \"No\" in ans) or (\"Yes\" not in ans and \"No\" not in ans):\n",
        "                gen = {\"knowledge\": knowledge, \"question\": question, \"answer\": answer, \"ground_truth\": ground_truth, \"judgement\": \"failed!\"}\n",
        "                dump_jsonl(gen, output_path, append=True)\n",
        "                incorrect += 1\n",
        "                print('sample {} fails......'.format(i))\n",
        "                continue\n",
        "            elif \"Yes\" in ans:\n",
        "                if ans != \"Yes\":\n",
        "                    ans = \"Yes\"\n",
        "                gen = {\"knowledge\": knowledge, \"question\": question, \"answer\": answer, \"ground_truth\": ground_truth, \"judgement\": ans}\n",
        "            elif \"No\" in ans:\n",
        "                if ans != \"No\":\n",
        "                    ans = \"No\"\n",
        "                gen = {\"knowledge\": knowledge, \"question\": question, \"answer\": answer, \"ground_truth\": ground_truth, \"judgement\": ans}\n",
        "            else:\n",
        "                gen = None\n",
        "                incorrect += 1\n",
        "\n",
        "            assert(gen is not None)\n",
        "\n",
        "            if ground_truth == ans:\n",
        "                correct += 1\n",
        "            else:\n",
        "                incorrect += 1\n",
        "\n",
        "            print('sample {} success......'.format(i))\n",
        "            dump_jsonl(gen, output_path, append=True)\n",
        "\n",
        "        print('{} correct samples, {} incorrect samples, Accuracy: {}'.format(correct, incorrect, correct/len(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwXUWPIGg8uQ"
      },
      "outputs": [],
      "source": [
        "#Evaluate the response generated\n",
        "\n",
        "def evaluate_responses(out_q, model, tokenizer):\n",
        "  evaluation_results = []\n",
        "  for response, judgment in out_q:\n",
        "    # You can add more sophisticated evaluation criteria here.\n",
        "    # For example, compare the generated response with a ground truth or a human evaluation.\n",
        "    correct = 0\n",
        "        incorrect = 0\n",
        "        for i in range(len(data)):\n",
        "            knowledge = data[i][\"knowledge\"]\n",
        "            question = data[i][\"question\"]\n",
        "            hallucinated_answer = data[i][\"hallucinated_answer\"]\n",
        "            right_answer = data[i][\"right_answer\"]\n",
        "\n",
        "            if random.random() > 0.5:\n",
        "                answer = hallucinated_answer\n",
        "                ground_truth = \"Yes\"\n",
        "            else:\n",
        "                answer = right_answer\n",
        "                ground_truth = \"No\"\n",
        "\n",
        "            ans = get_qa_response(model, question, answer, instruction)\n",
        "            ans = ans.replace(\".\", \"\")\n",
        "\n",
        "            if (\"Yes\" in ans and \"No\" in ans) or (\"Yes\" not in ans and \"No\" not in ans):\n",
        "                gen = {\"knowledge\": knowledge, \"question\": question, \"answer\": answer, \"ground_truth\": ground_truth, \"judgement\": \"failed!\"}\n",
        "                dump_jsonl(gen, output_path, append=True)\n",
        "                incorrect += 1\n",
        "                print('sample {} fails......'.format(i))\n",
        "                continue\n",
        "            elif \"Yes\" in ans:\n",
        "                if ans != \"Yes\":\n",
        "                    ans = \"Yes\"\n",
        "                gen = {\"knowledge\": knowledge, \"question\": question, \"answer\": answer, \"ground_truth\": ground_truth, \"judgement\": ans}\n",
        "            elif \"No\" in ans:\n",
        "                if ans != \"No\":\n",
        "                    ans = \"No\"\n",
        "                gen = {\"knowledge\": knowledge, \"question\": question, \"answer\": answer, \"ground_truth\": ground_truth, \"judgement\": ans}\n",
        "            else:\n",
        "                gen = None\n",
        "                incorrect += 1\n",
        "\n",
        "            assert(gen is not None)\n",
        "\n",
        "            if ground_truth == ans:\n",
        "                correct += 1\n",
        "            else:\n",
        "                incorrect += 1\n",
        "\n",
        "            print('sample {} success......'.format(i))\n",
        "            dump_jsonl(gen, output_path, append=True)\n",
        "\n",
        "        print('{} correct samples, {} incorrect samples, Accuracy: {}'.format(correct, incorrect, correct/len(data)))\n",
        "\n",
        "    evaluation_results.append((response, judgment, evaluation))\n",
        "  return evaluation_results\n",
        "  # Print the evaluation results\n",
        "  for response, judgment, evaluation in evaluation_results:\n",
        "    print(f\"Response: {response}\")\n",
        "    print(f\"Judgment: {judgment}\")\n",
        "    print(f\"Evaluation: {evaluation}\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnHLbYIWV2PN"
      },
      "outputs": [],
      "source": [
        "#evaluate response (with GPT answers)\n",
        "\"\"\"def Knowh_Eval(question, Info, answer):\n",
        "    prompt = \"system, instruction, know, question, GPTanswer(Hallucinator or correct), Response YES or NO\"\n",
        "    response = get_response(prompt_i)\n",
        "    prompt = \"System, Instruc, know, question, response, response\"\n",
        "    result = get_response(prompt)\n",
        "    return result\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}